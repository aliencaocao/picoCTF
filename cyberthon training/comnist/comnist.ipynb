{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_hub as hub\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Found 3081 files belonging to 34 classes.\n",
      "Using 2465 files for training.\n",
      "Validation data:\n",
      "Found 3081 files belonging to 34 classes.\n",
      "Using 616 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# data loading config\n",
    "batch_size = 64\n",
    "img_height = 100\n",
    "img_width = 100\n",
    "labels = 'inferred'\n",
    "label_mode = 'categorical'  # sparse one hot encoding\n",
    "color_mode = 'rgb'\n",
    "shuffle = True\n",
    "seed = 69\n",
    "val_split = 0.2\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "print('Training data:')\n",
    "train = tf.keras.utils.image_dataset_from_directory('train', labels=labels, label_mode=label_mode,\n",
    "color_mode=color_mode, shuffle=shuffle, seed=seed, image_size=(img_height, img_width), batch_size=batch_size, validation_split=val_split, subset='training')\n",
    "print('Validation data:')\n",
    "val = tf.keras.utils.image_dataset_from_directory('train', labels=labels, label_mode=label_mode,\n",
    "color_mode=color_mode, shuffle=shuffle, seed=seed, image_size=(img_height, img_width), batch_size=batch_size, validation_split=val_split, subset='validation')\n",
    "train_class_names = train.class_names\n",
    "\n",
    "train = train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val = val.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 1280)              53150388  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 34)                8738      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,552,854\n",
      "Trainable params: 53,260,822\n",
      "Non-trainable params: 292,032\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "39/39 [==============================] - 106s 577ms/step - loss: 8.5324 - accuracy: 0.3051 - val_loss: 6.4400 - val_accuracy: 0.6006 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - 80s 2s/step - loss: 5.2802 - accuracy: 0.7371 - val_loss: 4.2523 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - 10s 250ms/step - loss: 3.6164 - accuracy: 0.8686 - val_loss: 3.0461 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - 10s 252ms/step - loss: 2.5969 - accuracy: 0.9290 - val_loss: 2.2892 - val_accuracy: 0.9026 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - 10s 253ms/step - loss: 2.0096 - accuracy: 0.9456 - val_loss: 1.8822 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - 10s 247ms/step - loss: 1.6852 - accuracy: 0.9440 - val_loss: 1.6371 - val_accuracy: 0.9010 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "39/39 [==============================] - 10s 251ms/step - loss: 1.4375 - accuracy: 0.9566 - val_loss: 1.4135 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - 10s 250ms/step - loss: 1.2543 - accuracy: 0.9724 - val_loss: 1.3089 - val_accuracy: 0.9253 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "39/39 [==============================] - 10s 258ms/step - loss: 1.1434 - accuracy: 0.9801 - val_loss: 1.1757 - val_accuracy: 0.9481 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "39/39 [==============================] - 10s 255ms/step - loss: 1.0918 - accuracy: 0.9769 - val_loss: 1.2279 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "39/39 [==============================] - 10s 254ms/step - loss: 1.0613 - accuracy: 0.9716 - val_loss: 1.0958 - val_accuracy: 0.9351 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.9785\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "39/39 [==============================] - 10s 250ms/step - loss: 1.0128 - accuracy: 0.9785 - val_loss: 1.0895 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - 10s 255ms/step - loss: 0.9474 - accuracy: 0.9923 - val_loss: 1.0335 - val_accuracy: 0.9448 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - 10s 254ms/step - loss: 0.9152 - accuracy: 0.9976 - val_loss: 1.0102 - val_accuracy: 0.9497 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "39/39 [==============================] - 10s 252ms/step - loss: 0.8980 - accuracy: 0.9996 - val_loss: 1.0016 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "39/39 [==============================] - 10s 248ms/step - loss: 0.8947 - accuracy: 0.9992 - val_loss: 0.9909 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - 10s 251ms/step - loss: 0.8796 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - 10s 247ms/step - loss: 0.8769 - accuracy: 0.9996 - val_loss: 0.9707 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - 10s 249ms/step - loss: 0.8718 - accuracy: 0.9996 - val_loss: 0.9697 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "39/39 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 1.0000\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "39/39 [==============================] - 9s 244ms/step - loss: 0.8642 - accuracy: 1.0000 - val_loss: 0.9634 - val_accuracy: 0.9513 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "39/39 [==============================] - 10s 245ms/step - loss: 0.8593 - accuracy: 0.9996 - val_loss: 0.9614 - val_accuracy: 0.9513 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "39/39 [==============================] - 10s 250ms/step - loss: 0.8599 - accuracy: 1.0000 - val_loss: 0.9600 - val_accuracy: 0.9513 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1c3183db7f0>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model layers\n",
    "xIn = Input((img_height, img_width, 3))\n",
    "# x = tf.keras.layers.RandomFlip()(xIn)\n",
    "# effnet = tf.keras.applications.efficientnet_v2.EfficientNetV2M(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3),)\n",
    "effnet = hub.KerasLayer('https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2', trainable=True)  # no 1k-ft: 9648, with 1kft: 9655\n",
    "x = effnet(xIn)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(256, activation='swish', kernel_regularizer='l2')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(256, activation='swish', kernel_regularizer='l2')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "xOut = Dense(len(train_class_names), dtype=tf.float32)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=xIn, outputs=xOut)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "model.summary()\n",
    "model.fit(train, epochs=100, batch_size=64, validation_data=val, callbacks=callbacks, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 894). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: comnist_full_21k_ft1k_withval\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: comnist_full_21k_ft1k_withval\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('comnist_full_21k_ft1k_withval')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "model = load_model('comnist')  # 96.65\n",
    "model2 = load_model('comnist_full_21k_ft1k')  # 96.55\n",
    "model3 = load_model('comnist_1')  # 96.21\n",
    "model4 = load_model('comnist_full')  # 96.4\n",
    "model5 = load_model('comnist_full_21k')  # 96.48\n",
    "model6 = load_model('comnist_aug')  # 94.55"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12399 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test = tf.keras.utils.image_dataset_from_directory('test', labels=None, label_mode=None,\n",
    "color_mode=color_mode, shuffle=False, seed=seed, image_size=(img_height, img_width), batch_size=batch_size)\n",
    "filenames = test.file_paths\n",
    "filenames = [file.split('\\\\')[-1].split('.jpg')[0] for file in filenames]\n",
    "test = test.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "preds = model.predict(test)\n",
    "preds = tf.nn.softmax(preds)\n",
    "preds2 = model2.predict(test)\n",
    "preds2 = tf.nn.softmax(preds2)\n",
    "preds3 = model3.predict(test)\n",
    "preds3 = tf.nn.softmax(preds3)\n",
    "preds4 = model4.predict(test)\n",
    "preds4 = tf.nn.softmax(preds4)\n",
    "preds5 = model5.predict(test)\n",
    "preds5 = tf.nn.softmax(preds5)\n",
    "preds6 = model6.predict(test)\n",
    "preds6 = tf.nn.softmax(preds6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "preds_comb = preds * 0.18 + preds2 * 0.06 + preds3 * 0.13 + preds4 * 0.24 + preds5 * 0.2 + preds6 * 0.2  # preds * 0.18 + preds2 * 0.06 + preds3 * 0.13 + preds4 * 0.24 + preds5 * 0.2 + preds6 * 0.2: 9771\n",
    "preds_comb = tf.argmax(preds_comb, axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "preds_dict = dict()\n",
    "for i in range(len(preds)):\n",
    "    preds_dict.update({filenames[i]: train_class_names[int(preds_comb[i])]})\n",
    "\n",
    "pred_df = pd.read_csv('test.csv')\n",
    "pred_df_unordered = pd.DataFrame.from_dict(preds_dict, columns=['label'], orient='index')\n",
    "pred_df_unordered.reset_index(inplace=True)\n",
    "pred_df_unordered.columns = ['id', 'label']\n",
    "\n",
    "pred_df_aligned = pred_df.merge(pred_df_unordered, on='id', how='left')\n",
    "pred_df_aligned.drop(['label_x'], axis=1, inplace=True)\n",
    "pred_df_aligned.columns = ['id', 'label']\n",
    "pred_df_aligned.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSGraderClient: Successfully Connected!\n",
      "[SERVER] MOTD: CHECK your USER_TOKEN and GRADER_URL HTTP address! I'm CoMNIST @4d907f5f7099\n",
      "ProofOfWork Challenge =>  ('CTFSGRBab3a1774f892d5ca347a6b656ee5187b', 22)\n",
      "ProofOfWork Answer Found! =>  416133\n"
     ]
    },
    {
     "data": {
      "text/plain": "'{\"challenge\":{\"name\":\"CoMNIST\"},\"id\":\"cl269gr0relwu0832pweqcsgu\",\"status\":\"PARTIALLY_CORRECT\",\"multiplier\":0.9771,\"submittedBy\":{\"username\":\"hci-69\"},\"createdAt\":\"2022-04-19T14:48:25Z\"}'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to graders\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, 'C:/Users/alien/Documents/PyCharm Projects/Cyberthon 2021/pyctfsglib.py')\n",
    "import pyctfsglib as ctfsg\n",
    "import random\n",
    "\n",
    "USER_TOKEN = \"WrlLCkymxwtgFwRHZsdmKfSwcdqIpnqoXEtRkciVRZJfBJUgcEJoxVZjNTQRdqkR\" # You need to fill this up\n",
    "GRADER_URL = random.choice([\n",
    "  \"http://chals.cyberthon22t.ctf.sg:50201/\",\n",
    "  \"http://chals.cyberthon22t.ctf.sg:50202/\"\n",
    "])\n",
    "\n",
    "grader = ctfsg.DSGraderClient(GRADER_URL, USER_TOKEN)\n",
    "grader.submitFile('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}