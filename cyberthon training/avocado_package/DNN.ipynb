{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12774, 2)\n",
      "(12774, 54)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv('avocado-train.csv', index_col='id')\n",
    "df_train = df_train.dropna(axis=0)\n",
    "\n",
    "# Convert strings to numbers\n",
    "types = {'conventional': 0, 'organic': 1}\n",
    "df_train.type = [types[x] for x in df_train.type]\n",
    "regions = df_train.region.unique()\n",
    "regionDict = {x: i for i, x in enumerate(regions)}\n",
    "df_train.region = [regionDict[x] for x in df_train.region]\n",
    "\n",
    "types = df_train.pop('type')\n",
    "regions = df_train.pop('region')\n",
    "\n",
    "types = to_categorical(types)\n",
    "regions = to_categorical(regions)\n",
    "print(types.shape)\n",
    "print(regions.shape)\n",
    "\n",
    "df_train.drop(['XLargeBags'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def clean_date(x):\n",
    "    return float(x.replace('-',''))\n",
    "\n",
    "\n",
    "df_train['Date'] = df_train['Date'].apply(clean_date)\n",
    "\n",
    "df_train = pd.concat([df_train, pd.DataFrame(types), pd.DataFrame(regions)], axis=1)\n",
    "df_train.astype(float, copy=False)\n",
    "y_train = df_train.pop('AveragePrice')\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# x_scaler = MinMaxScaler(feature_range=(0, 1))  # scale between 0 and 1\n",
    "# x_scaler.fit(df_train)\n",
    "#\n",
    "# y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# y_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# df_train = x_scaler.transform(df_train)\n",
    "x_train = df_train.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 65)]              0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 128)               8448      \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,577\n",
      "Trainable params: 8,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xIn = Input(shape=(x_train.shape[1],))\n",
    "x = Dense(128, activation='swish')(xIn)\n",
    "xOut = Dense(1, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=xIn, outputs=xOut)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-2), loss='mse', metrics=['mse'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 1s 2ms/step - loss: 99978813440.0000 - mse: 99978813440.0000 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8046774272.0000 - mse: 8046774272.0000 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 8140194304.0000 - mse: 8140194304.0000 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2850561280.0000 - mse: 2850561280.0000 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 220919520.0000 - mse: 220919520.0000 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 176458352.0000 - mse: 176458352.0000 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 64453644.0000 - mse: 64453644.0000 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 237881568.0000 - mse: 237881568.0000 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 8910203904.0000 - mse: 8910203904.0000 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 103153816.0000 - mse: 103153816.0000 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 42448980.0000 - mse: 42448980.0000 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 35248856.0000 - mse: 35248856.0000 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 20630364.0000 - mse: 20630364.0000 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 13587124.0000 - mse: 13587124.0000 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 9357893.0000 - mse: 9357893.0000 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 778393984.0000 - mse: 778393984.0000 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 90741496.0000 - mse: 90741496.0000 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 912229120.0000 - mse: 912229120.0000 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 53979556.0000 - mse: 53979556.0000 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 784324160.0000 - mse: 784324160.0000 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 22255736.0000 - mse: 22255736.0000 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1951750784.0000 - mse: 1951750784.0000 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 146860864.0000 - mse: 146860864.0000 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 12957544.0000 - mse: 12957544.0000 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4286816.5000 - mse: 4286816.5000 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3345781.0000 - mse: 3345781.0000 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 13865562.0000 - mse: 13865562.0000 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 138772288.0000 - mse: 138772288.0000 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 299181024.0000 - mse: 299181024.0000 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 20375518.0000 - mse: 20375518.0000 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 173067856.0000 - mse: 173067856.0000 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 541217472.0000 - mse: 541217472.0000 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3607697.5000 - mse: 3607697.5000 - lr: 0.0100\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1802892.2500 - mse: 1802892.2500 - lr: 0.0100\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1159166.5000 - mse: 1159166.5000 - lr: 0.0100\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 880418.5000 - mse: 880418.5000 - lr: 0.0100\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 51669368.0000 - mse: 51669368.0000 - lr: 0.0100\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 67822744.0000 - mse: 67822744.0000 - lr: 0.0100\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 999846.4375 - mse: 999846.4375 - lr: 0.0100\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 62195900.0000 - mse: 62195900.0000 - lr: 0.0100\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 5078634.5000 - mse: 5078634.5000 - lr: 0.0100\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 24563862.0000 - mse: 24563862.0000 - lr: 0.0100\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 43282460.0000 - mse: 43282460.0000 - lr: 0.0100\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 32948266.0000 - mse: 32948266.0000 - lr: 0.0100\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 423038.4062 - mse: 423038.4062 - lr: 0.0100\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 9925477.0000 - mse: 9925477.0000 - lr: 0.0100\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 40814644.0000 - mse: 40814644.0000 - lr: 0.0100\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 230258.8594 - mse: 230258.8594 - lr: 0.0100\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 439065.6250 - mse: 439065.6250 - lr: 0.0100\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 8766803.0000 - mse: 8766803.0000 - lr: 0.0100\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 25733752.0000 - mse: 25733752.0000 - lr: 0.0100\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 168665.9688 - mse: 168665.9688 - lr: 0.0100\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 217249.6250 - mse: 217249.6250 - lr: 0.0100\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 5625153.0000 - mse: 5625153.0000 - lr: 0.0100\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4020784.7500 - mse: 4020784.7500 - lr: 0.0100\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 7848670.5000 - mse: 7848670.5000 - lr: 0.0100\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 177926.0156 - mse: 177926.0156 - lr: 0.0100\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 5809897.5000 - mse: 5809897.5000 - lr: 0.0100\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 84360.2500 - mse: 84360.2500 - lr: 0.0100\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1801546.0000 - mse: 1801546.0000 - lr: 0.0100\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 9247471.0000 - mse: 9247471.0000 - lr: 0.0100\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 15584.3408 - mse: 15584.3408 - lr: 0.0100\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 12475.7529 - mse: 12475.7529 - lr: 0.0100\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 11392.7744 - mse: 11392.7744 - lr: 0.0100\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 14818.4248 - mse: 14818.4248 - lr: 0.0100\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 946607.6250 - mse: 946607.6250 - lr: 0.0100\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1614333.0000 - mse: 1614333.0000 - lr: 0.0100\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 12406.9355 - mse: 12406.9355 - lr: 0.0100\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3185495.2500 - mse: 3185495.2500 - lr: 0.0100\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 14389.1475 - mse: 14389.1475 - lr: 0.0100\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3267.2100 - mse: 3267.2100 - lr: 0.0100\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2094.9421 - mse: 2094.9421 - lr: 0.0100\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1542.6875 - mse: 1542.6875 - lr: 0.0100\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1466.7152 - mse: 1466.7152 - lr: 0.0100\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1415.6492 - mse: 1415.6492 - lr: 0.0100\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 88185.0156 - mse: 88185.0156 - lr: 0.0100\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 156165.1250 - mse: 156165.1250 - lr: 0.0100\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 168110.2812 - mse: 168110.2812 - lr: 0.0100\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 315249.4062 - mse: 315249.4062 - lr: 0.0100\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 148133.9531 - mse: 148133.9531 - lr: 0.0100\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 287.6685 - mse: 287.6685 - lr: 0.0100\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 238.7980 - mse: 238.7980 - lr: 0.0100\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 301.4374 - mse: 301.4374 - lr: 0.0100\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 885.9690 - mse: 885.9690 - lr: 0.0100\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 110269.8281 - mse: 110269.8281 - lr: 0.0100\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 122.3263 - mse: 122.3263 - lr: 0.0100\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1595.7570 - mse: 1595.7570 - lr: 0.0100\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 158639.8906 - mse: 158639.8906 - lr: 0.0100\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 22.1733 - mse: 22.1733 - lr: 0.0100\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 20.6851 - mse: 20.6851 - lr: 0.0100\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 14.2230 - mse: 14.2230 - lr: 0.0100\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 14.8246 - mse: 14.8246 - lr: 0.0100\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 26.7548 - mse: 26.7548 - lr: 0.0100\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 65.0962 - mse: 65.0962 - lr: 0.0100\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 47754.0508 - mse: 47754.0508 - lr: 0.0100\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.5554 - mse: 3.5554 - lr: 0.0100\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.9273 - mse: 2.9273 - lr: 0.0100\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.7680 - mse: 3.7680 - lr: 0.0100\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4.3679 - mse: 4.3679 - lr: 0.0100\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4.3299 - mse: 4.3299 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1f35cee8f10>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='mse', factor=0.1, patience=15, verbose=1),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor='mse', patience=40, verbose=1, restore_best_weights=True)]\n",
    "\n",
    "\n",
    "model.fit(np.asarray(x_train), np.asarray(y_train), epochs=100, batch_size=64, shuffle=True, callbacks=callbacks, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5475, 2)\n",
      "(5475, 54)\n"
     ]
    },
    {
     "data": {
      "text/plain": "      DataBatch        Date  TotalVolume      4046       4225     4770  \\\n0          21.0  20160731.0     28969.34     80.77   27361.91     0.00   \n1          19.0  20170820.0    322962.89   5632.85  242365.82  2441.56   \n2          49.0  20170122.0    155334.45   3657.79   74068.65     0.00   \n3          12.0  20151004.0     10231.74    341.89    8519.00     0.00   \n4          10.0  20151018.0     10652.02   6905.95      39.52     0.00   \n...         ...         ...          ...       ...        ...      ...   \n5470        8.0  20171105.0     10865.72   1262.85    2499.88    52.94   \n5471       47.0  20150201.0     57424.15   1321.47   30243.88    14.24   \n5472        8.0  20171105.0   1319642.14  61830.21  632293.37   600.50   \n5473       14.0  20160918.0      9663.48    220.83     194.94    22.98   \n5474       51.0  20150104.0     58065.35  10049.66   25228.37  3672.89   \n\n      TotalBags  SmallBags  LargeBags    0  ...   44   45   46   47   48   49  \\\n0       1526.66    1526.66       0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n1      72522.66   22942.48   46083.51  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n2      77608.01   27302.29   50305.72  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n3       1370.85     303.33    1067.52  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n4       3706.55    3706.55       0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...        ...        ...  ...  ...  ...  ...  ...  ...  ...  ...   \n5470    7050.05    6880.75     169.30  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n5471   25844.56   25793.45      51.11  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n5472  624918.06  563910.07   60984.25  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n5473    9224.73    9220.50       4.23  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n5474   19114.43   17280.89    1833.54  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       50   51   52   53  \n0     0.0  0.0  0.0  0.0  \n1     0.0  0.0  0.0  0.0  \n2     0.0  0.0  0.0  0.0  \n3     0.0  0.0  0.0  0.0  \n4     0.0  0.0  0.0  0.0  \n...   ...  ...  ...  ...  \n5470  1.0  0.0  0.0  0.0  \n5471  0.0  0.0  0.0  0.0  \n5472  0.0  0.0  0.0  0.0  \n5473  0.0  0.0  0.0  0.0  \n5474  0.0  0.0  0.0  0.0  \n\n[5475 rows x 65 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DataBatch</th>\n      <th>Date</th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>0</th>\n      <th>...</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21.0</td>\n      <td>20160731.0</td>\n      <td>28969.34</td>\n      <td>80.77</td>\n      <td>27361.91</td>\n      <td>0.00</td>\n      <td>1526.66</td>\n      <td>1526.66</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.0</td>\n      <td>20170820.0</td>\n      <td>322962.89</td>\n      <td>5632.85</td>\n      <td>242365.82</td>\n      <td>2441.56</td>\n      <td>72522.66</td>\n      <td>22942.48</td>\n      <td>46083.51</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49.0</td>\n      <td>20170122.0</td>\n      <td>155334.45</td>\n      <td>3657.79</td>\n      <td>74068.65</td>\n      <td>0.00</td>\n      <td>77608.01</td>\n      <td>27302.29</td>\n      <td>50305.72</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12.0</td>\n      <td>20151004.0</td>\n      <td>10231.74</td>\n      <td>341.89</td>\n      <td>8519.00</td>\n      <td>0.00</td>\n      <td>1370.85</td>\n      <td>303.33</td>\n      <td>1067.52</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.0</td>\n      <td>20151018.0</td>\n      <td>10652.02</td>\n      <td>6905.95</td>\n      <td>39.52</td>\n      <td>0.00</td>\n      <td>3706.55</td>\n      <td>3706.55</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5470</th>\n      <td>8.0</td>\n      <td>20171105.0</td>\n      <td>10865.72</td>\n      <td>1262.85</td>\n      <td>2499.88</td>\n      <td>52.94</td>\n      <td>7050.05</td>\n      <td>6880.75</td>\n      <td>169.30</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5471</th>\n      <td>47.0</td>\n      <td>20150201.0</td>\n      <td>57424.15</td>\n      <td>1321.47</td>\n      <td>30243.88</td>\n      <td>14.24</td>\n      <td>25844.56</td>\n      <td>25793.45</td>\n      <td>51.11</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5472</th>\n      <td>8.0</td>\n      <td>20171105.0</td>\n      <td>1319642.14</td>\n      <td>61830.21</td>\n      <td>632293.37</td>\n      <td>600.50</td>\n      <td>624918.06</td>\n      <td>563910.07</td>\n      <td>60984.25</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5473</th>\n      <td>14.0</td>\n      <td>20160918.0</td>\n      <td>9663.48</td>\n      <td>220.83</td>\n      <td>194.94</td>\n      <td>22.98</td>\n      <td>9224.73</td>\n      <td>9220.50</td>\n      <td>4.23</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5474</th>\n      <td>51.0</td>\n      <td>20150104.0</td>\n      <td>58065.35</td>\n      <td>10049.66</td>\n      <td>25228.37</td>\n      <td>3672.89</td>\n      <td>19114.43</td>\n      <td>17280.89</td>\n      <td>1833.54</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5475 rows × 65 columns</p>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df_test = pd.read_csv('avocado-test.csv', index_col='id')\n",
    "df_test = df_test.dropna(axis=0)\n",
    "\n",
    "# Convert strings to numbers\n",
    "types = {'conventional': 0, 'organic': 1}\n",
    "df_test.type = [types[x] for x in df_test.type]\n",
    "regions = df_test.region.unique()\n",
    "regionDict = {x: i for i, x in enumerate(regions)}\n",
    "df_test.region = [regionDict[x] for x in df_test.region]\n",
    "\n",
    "types = df_test.pop('type')\n",
    "regions = df_test.pop('region')\n",
    "\n",
    "types = to_categorical(types)\n",
    "regions = to_categorical(regions)\n",
    "print(types.shape)\n",
    "print(regions.shape)\n",
    "\n",
    "df_test.drop(['XLargeBags'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def clean_date(x):\n",
    "    return float(x.replace('-',''))\n",
    "\n",
    "\n",
    "df_test['Date'] = df_test['Date'].apply(clean_date)\n",
    "\n",
    "df_test = pd.concat([df_test, pd.DataFrame(types), pd.DataFrame(regions)], axis=1)\n",
    "df_test.astype(float, copy=False)\n",
    "\n",
    "# df_test = pd.DataFrame(x_scaler.transform(df_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  AveragePrice\n",
      "0        0      1.331390\n",
      "1        1      3.128998\n",
      "2        2      1.982269\n",
      "3        3      1.289398\n",
      "4        4      1.023773\n",
      "...    ...           ...\n",
      "5470  5470      1.044281\n",
      "5471  5471      0.954925\n",
      "5472  5472      0.362101\n",
      "5473  5473      1.490295\n",
      "5474  5474      1.184128\n",
      "\n",
      "[5475 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(np.asarray(df_test))\n",
    "# preds = y_scaler.inverse_transform(preds)\n",
    "preds = pd.DataFrame(preds)\n",
    "output = pd.DataFrame({'id': list(range(len(df_test))), 'AveragePrice': np.squeeze(preds)})\n",
    "output.to_csv('avocado-submission-dnn.csv', index=False)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded pyctfsglib.py: True\n",
      "DSGraderClient: Successfully Connected!\n",
      "[SERVER] MOTD: CHECK your USER_TOKEN and GRADER_URL HTTP address! I'm AVOCADO_PRICE @366389bec30e\n",
      "ProofOfWork Challenge =>  ('CTFSGRB6778d6e15d2feba854cf27fdfa281023', 22)\n",
      "ProofOfWork Answer Found! =>  28645939\n"
     ]
    },
    {
     "data": {
      "text/plain": "'{\"challenge\":{\"name\":\"Avocado Pricing\"},\"id\":\"cl21rhuaabo1r0763a9v07hl7\",\"status\":\"PARTIALLY_CORRECT\",\"multiplier\":0,\"submittedBy\":{\"username\":\"hci-69\"},\"createdAt\":\"2022-04-16T11:14:18Z\"}'"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download CTFSG Grader Libraries\n",
    "import urllib.request, os\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/alttablabs/ctfsg-utils/master/pyctfsglib.py', './pyctfsglib.py')\n",
    "print('Downloaded pyctfsglib.py:', 'pyctfsglib.py' in os.listdir())\n",
    "# Connect to graders\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, 'C:/Users/alien/Documents/PyCharm Projects/Cyberthon 2021/pyctfsglib.py')\n",
    "import pyctfsglib as ctfsg\n",
    "import random\n",
    "\n",
    "USER_TOKEN = \"WrlLCkymxwtgFwRHZsdmKfSwcdqIpnqoXEtRkciVRZJfBJUgcEJoxVZjNTQRdqkR\" # You need to fill this up\n",
    "GRADER_URL = random.choice([\n",
    "  \"http://chals.cyberthon22t.ctf.sg:50101/\",\n",
    "  \"http://chals.cyberthon22t.ctf.sg:50102/\"\n",
    "])\n",
    "\n",
    "grader = ctfsg.DSGraderClient(GRADER_URL, USER_TOKEN)\n",
    "grader.submitFile('avocado-submission-dnn.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}